{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "24hackton_imageclassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqeFVTzX9Jnr",
        "colab_type": "text"
      },
      "source": [
        "##Import library & Load the dataset from gdrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiV_Pgy48-ZC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25b7219d-e6e2-41cc-9094-5132366625e5"
      },
      "source": [
        "# Import all the necessary library & files!\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.models  import Sequential, Model\n",
        "from keras.layers import Conv2D, Flatten, MaxPooling2D, Dense, GlobalAveragePooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG0eGKYw9qMh",
        "colab_type": "text"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl26RyCw9RH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_path = './gdrive/My Drive/24hackton/images'\n",
        "model_path = './gdrive/My Drive/24hackton/models'\n",
        "train_img_path = '{}/train'.format(image_path)\n",
        "val_img_path = '{}/val'.format(image_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZq9tsin9ko8",
        "colab_type": "text"
      },
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHS-hCB_9UB_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0afa041-b378-4d15-d2a3-5f85c7ed214e"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        rotation_range=25,\n",
        "        vertical_flip=True,\n",
        "        fill_mode='reflect',\n",
        "        data_format='channels_last',\n",
        "        brightness_range=[0.5, 1.5],\n",
        "        featurewise_center=True,\n",
        "        featurewise_std_normalization=True)\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    rescale=1/255)\n",
        "\n",
        "# train_generator = train_datagen.flow_from_directory(\n",
        "#     train_img_path,\n",
        "#     target_size=(224, 224),\n",
        "#     batch_size=64,\n",
        "#     class_mode='categorical')\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_img_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 232 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPFPcY-R3yJl",
        "colab_type": "text"
      },
      "source": [
        "###Load the train images from folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6tPqhRO90Ny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_img(filepath, size):\n",
        "    '''\n",
        "    Load single image from file.\n",
        "    :param filepath: Path of the image.\n",
        "    :size: size to resize\n",
        "    '''\n",
        "    img = load_img(filepath, target_size=size)\n",
        "    img = img_to_array(img)\n",
        "    img = np.array(img)/255.0\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "images = []\n",
        "lables = []\n",
        "\n",
        "for category_id, class_label in enumerate(val_generator.class_indices):\n",
        "    print(class_label)\n",
        "    print(category_id)\n",
        "    train_img_path = '{}/train/{}'.format(image_path, class_label)\n",
        "    for filename in os.listdir(train_img_path):\n",
        "        images.append(get_img(os.path.join(\n",
        "            train_img_path, filename), (224, 224)))\n",
        "        lables.append(category_id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HF5pyyF4kpp",
        "colab_type": "text"
      },
      "source": [
        "### Apply SMOTE on the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUS9_ah74aKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "counter = Counter(lables)\n",
        "print(counter)\n",
        "\n",
        "images = np.array(images)\n",
        "print(images.shape)\n",
        "n_sample, b, c, d = images.shape\n",
        "images = images.reshape(n_sample, b*c*d)\n",
        "\n",
        "oversample = SMOTE()\n",
        "X, y = oversample.fit_resample(images, lables)\n",
        "new_n_sample= (X.shape)[0]\n",
        "\n",
        "X = X.reshape(new_n_sample, b, c, d)\n",
        "y = to_categorical(y, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "075EjLiiN9Qc",
        "colab_type": "text"
      },
      "source": [
        "## Building of CNN Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1rv_Z07JBp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.inception_v3 import InceptionV3 \n",
        "\n",
        "def create_model(dropout= True):\n",
        "  base_model = InceptionV3(weights='imagenet', include_top=False)\n",
        "\n",
        "  # add a global spatial average pooling layer\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "  # # let's add a fully-connected layer\n",
        "  predictions = Dense(5, activation='softmax')(x)\n",
        "\n",
        "  # this is the model we will train\n",
        "  model = Model(inputs=base_model.input, outputs=predictions)\n",
        "  # model.summary()\n",
        "\n",
        "  # freeze all convolutional layers\n",
        "  for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  return model\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "# Save the best model with max val_acc using ModelCheckpoint\n",
        "checkpoint1 = ModelCheckpoint(\"{}/model.h5\".format(model_path), monitor='val_acc', verbose=3, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint1]\n",
        "\n",
        "# Define parameters\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkAAcboBUS0h",
        "colab_type": "text"
      },
      "source": [
        "### Train of model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxvmXsfKTr4V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "29be3d25-02c6-47b3-cd1e-9a7d6e503b6e"
      },
      "source": [
        "history = model.fit_generator(\n",
        "    train_datagen.flow(X, y, batch_size=32),\n",
        "    epochs=10,\n",
        "    steps_per_epoch=new_n_sample//32,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.n//val_generator.batch_size,\n",
        "    callbacks=callbacks_list) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:724: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "102/102 [==============================] - 290s 3s/step - loss: 0.9582 - acc: 0.6685 - val_loss: 0.4282 - val_acc: 0.8490\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.84896, saving model to ./gdrive/My Drive/24hackton/models/model.h5\n",
            "Epoch 2/10\n",
            "102/102 [==============================] - 284s 3s/step - loss: 0.5319 - acc: 0.8284 - val_loss: 0.3509 - val_acc: 0.8631\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.84896 to 0.86310, saving model to ./gdrive/My Drive/24hackton/models/model.h5\n",
            "Epoch 3/10\n",
            "102/102 [==============================] - 285s 3s/step - loss: 0.4389 - acc: 0.8637 - val_loss: 0.4105 - val_acc: 0.8095\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86310\n",
            "Epoch 4/10\n",
            "102/102 [==============================] - 285s 3s/step - loss: 0.3805 - acc: 0.8793 - val_loss: 0.3294 - val_acc: 0.8810\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.86310 to 0.88095, saving model to ./gdrive/My Drive/24hackton/models/model.h5\n",
            "Epoch 5/10\n",
            "102/102 [==============================] - 288s 3s/step - loss: 0.3485 - acc: 0.8854 - val_loss: 0.3642 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.88095\n",
            "Epoch 6/10\n",
            "102/102 [==============================] - 288s 3s/step - loss: 0.3395 - acc: 0.8848 - val_loss: 0.4569 - val_acc: 0.8452\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.88095\n",
            "Epoch 7/10\n",
            "102/102 [==============================] - 285s 3s/step - loss: 0.2988 - acc: 0.9023 - val_loss: 0.2674 - val_acc: 0.9048\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.88095 to 0.90476, saving model to ./gdrive/My Drive/24hackton/models/model.h5\n",
            "Epoch 8/10\n",
            "102/102 [==============================] - 284s 3s/step - loss: 0.2970 - acc: 0.8995 - val_loss: 0.4635 - val_acc: 0.8452\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.90476\n",
            "Epoch 9/10\n",
            "102/102 [==============================] - 285s 3s/step - loss: 0.2833 - acc: 0.8998 - val_loss: 0.4846 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.90476\n",
            "Epoch 10/10\n",
            "102/102 [==============================] - 285s 3s/step - loss: 0.2821 - acc: 0.9059 - val_loss: 0.3549 - val_acc: 0.8810\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.90476\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}